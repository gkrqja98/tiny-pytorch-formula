<!DOCTYPE html>
<html>
<head>
<title>TinyCNN Model Forward and Backward Analysis</title>
<meta charset="UTF-8">
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML'></script>
<style>
body { font-family: Arial, sans-serif; margin: 20px; line-height: 1.6; }
h1 { color: #2c3e50; }
h2 { color: #3498db; border-bottom: 1px solid #3498db; padding-bottom: 5px; }
h3 { color: #2980b9; }
h4 { color: #27ae60; }
.layer-summary { margin-bottom: 30px; padding: 15px; border: 1px solid #ddd; border-radius: 5px; box-shadow: 0 2px 4px rgba(0,0,0,0.1); }
.forward-pass { border-left: 5px solid #2ecc71; padding-left: 15px; }
.backward-pass { border-left: 5px solid #e74c3c; padding-left: 15px; }
.tensor-table { border-collapse: collapse; margin: 10px 0; width: auto; }
.tensor-table th, .tensor-table td { border: 1px solid #ddd; padding: 8px; text-align: right; }
.tensor-table th { background-color: #f2f2f2; }
.formula { background-color: #f9f9f9; padding: 10px; border-radius: 5px; overflow-x: auto; }
</style>
</head>
<body>
<h1>Complete Forward and Backward Analysis of TinyCNN Model</h1>
<p><strong>Input Shape:</strong> torch.Size([1, 1, 8, 8])</p>
<p><strong>Output Shape:</strong> torch.Size([1, 2])</p>
<p><strong>Loss Value:</strong> 0.305365</p>
<div class='layer-summary'><h2>Layer: conv1 (Conv2d)</h2>

<p><strong>Input Shape</strong>: torch.Size([1, 1, 8, 8])</p>
<p><strong>Output Shape</strong>: torch.Size([1, 2, 8, 8])</p>

<h3>Forward Pass</h3>

<p><strong>General Formula</strong>: \(y_{n,c_{out},h_{out},w_{out}} = \sum_{c_{in},k_h,k_w} x_{n,c_{in},h_{in}+k_h,w_{in}+k_w} \cdot w_{c_{out},c_{in},k_h,k_w} + b_{c_{out}}\)</p>


    <div class="computation-details">
    
    <h3>Computing output at position: (batch=0, out_channel=0, y=1, x=1)</h3>
    
    <h4>Filter Weights (Shape: (1, 3, 3))</h4>
    <h5>Channel 0</h5>
<table class='tensor-table'>
<tr><th></th><th>[0]</th><th>[1]</th><th>[2]</th></tr>
<tr><th>[0]</th><td>0.3000</td><td>-0.5000</td><td>0.2000</td></tr>
<tr><th>[1]</th><td>0.7000</td><td>0.4000</td><td>-0.1000</td></tr>
<tr><th>[2]</th><td>-0.3000</td><td>0.8000</td><td>0.5000</td></tr>
</table>

    
    <h4>Input Receptive Field (Shape: (3, 3, 1))</h4>
    <h5>Channel 0</h5>
<table class='tensor-table'>
<tr><th></th><th>[0]</th></tr>
<tr><th>[0]</th><td>0.2000</td></tr>
<tr><th>[1]</th><td>-0.0286</td></tr>
<tr><th>[2]</th><td>0.3429</td></tr>
</table>
<h5>Channel 1</h5>
<table class='tensor-table'>
<tr><th></th><th>[0]</th></tr>
<tr><th>[0]</th><td>-0.0286</td></tr>
<tr><th>[1]</th><td>0.3429</td></tr>
<tr><th>[2]</th><td>0.1143</td></tr>
</table>
<h5>Channel 2</h5>
<table class='tensor-table'>
<tr><th></th><th>[0]</th></tr>
<tr><th>[0]</th><td>0.3429</td></tr>
<tr><th>[1]</th><td>0.1143</td></tr>
<tr><th>[2]</th><td>0.4857</td></tr>
</table>

    
    <h3>General Formula</h3>
    <p>\(y_{n,c_{out},h_{out},w_{out}} = \sum_{c_{in}} \sum_{k_h=0}^{K_h-1} \sum_{k_w=0}^{K_w-1} x_{n,c_{in},h_{in}+k_h,w_{in}+k_w} \cdot w_{c_{out},c_{in},k_h,k_w} + b_{c_{out}}\)</p>
    
    <h3>Value Substitution</h3>
    <p>\((0.2000 \times 0.3000) + (-0.0286 \times -0.5000) + (0.3429 \times 0.2000) + (-0.0286 \times 0.7000) + (0.3429 \times 0.4000) + (0.1143 \times -0.1000) + (0.3429 \times -0.3000) + (0.1143 \times 0.8000) + (0.4857 \times 0.5000) + 0.1000\)</p>
    
    <h3>Computation Result</h3>
    <p>Calculated value: 0.580000</p>
    <p>Actual output value: 0.580000</p>
    
</div><h3>Backward Pass</h3>

<p><strong>General Formula</strong>: \(\frac{\partial L}{\partial x_{n,c_{in},h_{in},w_{in}}} = \sum_{c_{out},k_h,k_w} \frac{\partial L}{\partial y_{n,c_{out},h_{out},w_{out}}} \cdot w_{c_{out},c_{in},k_h,k_w}\)</p>


    <div class="backward-computation">
    <h3>Backward Computation for Conv2d at Position: (batch=0, out_channel=0, y=1, x=1)</h3>
    
    <h4>Gradient Output Value</h4>
    <p>-0.003830</p>
    
    <h4>Filter Weights</h4>
    <h5>Channel 0</h5>
<table class='tensor-table'>
<tr><th></th><th>[0]</th><th>[1]</th><th>[2]</th></tr>
<tr><th>[0]</th><td>0.3000</td><td>-0.5000</td><td>0.2000</td></tr>
<tr><th>[1]</th><td>0.7000</td><td>0.4000</td><td>-0.1000</td></tr>
<tr><th>[2]</th><td>-0.3000</td><td>0.8000</td><td>0.5000</td></tr>
</table>

    
    <h4>Gradient Propagation to Input</h4>
    <table class="tensor-table">
    <tr><th>Position (y, x, c)</th><th>Formula</th><th>Gradient Value</th></tr>
    <tr><td>(0, 0, 0)</td><td>\((-0.0038 \times 0.3000)\)</td><td>-0.001149</td></tr>
<tr><td>(0, 1, 0)</td><td>\((-0.0038 \times -0.5000)\)</td><td>0.001915</td></tr>
<tr><td>(0, 2, 0)</td><td>\((-0.0038 \times 0.2000)\)</td><td>-0.000766</td></tr>
<tr><td>(1, 0, 0)</td><td>\((-0.0038 \times 0.7000)\)</td><td>-0.002681</td></tr>
<tr><td>(1, 1, 0)</td><td>\((-0.0038 \times 0.4000)\)</td><td>-0.001532</td></tr>
<tr><td>(1, 2, 0)</td><td>\((-0.0038 \times -0.1000)\)</td><td>0.000383</td></tr>
<tr><td>(2, 0, 0)</td><td>\((-0.0038 \times -0.3000)\)</td><td>0.001149</td></tr>
<tr><td>(2, 1, 0)</td><td>\((-0.0038 \times 0.8000)\)</td><td>-0.003064</td></tr>
<tr><td>(2, 2, 0)</td><td>\((-0.0038 \times 0.5000)\)</td><td>-0.001915</td></tr>

    </table>
    
    <h4>Weight Gradient Computation</h4>
    <table class="tensor-table">
    <tr><th>Weight Position (out_c, in_c, ky, kx)</th><th>Formula</th><th>Gradient Value</th></tr>
    <tr><td>(0, 0, 0, 0)</td><td>\((-0.0038 \times 0.2000)\)</td><td>-0.000766</td></tr>
<tr><td>(0, 0, 0, 1)</td><td>\((-0.0038 \times -0.0286)\)</td><td>0.000109</td></tr>
<tr><td>(0, 0, 0, 2)</td><td>\((-0.0038 \times 0.3429)\)</td><td>-0.001313</td></tr>
<tr><td>(0, 0, 1, 0)</td><td>\((-0.0038 \times -0.0286)\)</td><td>0.000109</td></tr>
<tr><td>(0, 0, 1, 1)</td><td>\((-0.0038 \times 0.3429)\)</td><td>-0.001313</td></tr>
<tr><td>(0, 0, 1, 2)</td><td>\((-0.0038 \times 0.1143)\)</td><td>-0.000438</td></tr>
<tr><td>(0, 0, 2, 0)</td><td>\((-0.0038 \times 0.3429)\)</td><td>-0.001313</td></tr>
<tr><td>(0, 0, 2, 1)</td><td>\((-0.0038 \times 0.1143)\)</td><td>-0.000438</td></tr>
<tr><td>(0, 0, 2, 2)</td><td>\((-0.0038 \times 0.4857)\)</td><td>-0.001860</td></tr>

    </table>
    
    <h4>Bias Gradient Computation</h4>
    <p>Bias Gradient for output channel 0: -0.003830</p>
    
    <h3>General Gradient Formulas</h3>
    
    <h4>Input Gradient Formula</h4>
    <p>\(\frac{\partial L}{\partial x_{n,c_{in},h_{in},w_{in}}} = \sum_{c_{out}} \sum_{k_h=0}^{K_h-1} \sum_{k_w=0}^{K_w-1} \frac{\partial L}{\partial y_{n,c_{out},h_{out},w_{out}}} \cdot w_{c_{out},c_{in},k_h,k_w}\)</p>
    
    <h4>Weight Gradient Formula</h4>
    <p>\(\frac{\partial L}{\partial w_{c_{out},c_{in},k_h,k_w}} = \sum_{n} \sum_{h_{out},w_{out}} \frac{\partial L}{\partial y_{n,c_{out},h_{out},w_{out}}} \cdot x_{n,c_{in},h_{in}+k_h,w_{in}+k_w}\)</p>
    
    <h4>Bias Gradient Formula</h4>
    <p>\(\frac{\partial L}{\partial b_{c_{out}}} = \sum_{n} \sum_{h_{out},w_{out}} \frac{\partial L}{\partial y_{n,c_{out},h_{out},w_{out}}}\)</p>
    </div>
    </div>

<div class='layer-summary'><h2>Layer: pool1 (MaxPool2d)</h2>

<p><strong>Input Shape</strong>: torch.Size([1, 2, 8, 8])</p>
<p><strong>Output Shape</strong>: torch.Size([1, 2, 4, 4])</p>

<h3>Forward Pass</h3>

<p><strong>General Formula</strong>: \(y_{n,c,h_{out},w_{out}} = \max_{k_h,k_w} x_{n,c,h_{in}+k_h,w_{in}+k_w}\)</p>


    <div class="computation-details">
    
    <h3>Computing output at position: (batch=0, channel=0, y=1, x=1)</h3>
    
    <h4>Input Receptive Field (Shape: (2, 2))</h4>
    <table class='tensor-table'>
<tr><th></th><th>[0]</th><th>[1]</th></tr>
<tr><th>[0]</th><td>1.2314</td><td>1.4557</td></tr>
<tr><th>[1]</th><td>1.7514</td><td>1.8643</td></tr>
</table>
    <p>Maximum value position in receptive field: (1, 1) with value 1.8643</p>
    
    <h3>General Formula</h3>
    <p>\(y_{n,c,h_{out},w_{out}} = \max_{0 \leq i < k_h, 0 \leq j < k_w} x_{n,c,h_{in}+i,w_{in}+j}\)</p>
    
    <h3>Value Substitution</h3>
    <p>\(\max(1.2314, 1.4557, 1.7514, 1.8643)\)</p>
    
    <h3>Computation Result</h3>
    <p>Calculated max value: 1.864286</p>
    <p>Actual output value: 1.864286</p>
    
</div><h3>Backward Pass</h3>

<p><strong>General Formula</strong>: \(\frac{\partial L}{\partial x_{n,c,h_{in},w_{in}}} = \begin{cases} \frac{\partial L}{\partial y_{n,c,h_{out},w_{out}}} & \text{if } x_{n,c,h_{in},w_{in}} \text{ is max in pool} \\ 0 & \text{otherwise} \end{cases}\)</p>


    <div class="backward-computation">
    <h3>Backward Computation for MaxPool2d at Position: (batch=0, channel=0, y=1, x=1)</h3>
    
    <h4>Gradient Output Value</h4>
    <p>-0.003542</p>
    
    <h4>Input Receptive Field</h4>
    <table class='tensor-table'>
<tr><th></th><th>[0]</th><th>[1]</th></tr>
<tr><th>[0]</th><td>1.2314</td><td>1.4557</td></tr>
<tr><th>[1]</th><td>1.7514</td><td>1.8643</td></tr>
</table>
    
    <p>Maximum value position in receptive field: (1, 1) with value 1.864286</p>
    
    <h4>Gradient Propagation to Input</h4>
    <table class="tensor-table">
    <tr><th>Position (y, x)</th><th>Formula</th><th>Gradient Value</th></tr>
    <tr><td>(3, 3)</td><td>\(-0.0035\)</td><td>-0.003542</td></tr>

    </table>
    
    <p>All other positions in the receptive field receive zero gradient.</p>
    
    <h4>Gradient Map (Receptive Field)</h4>
    <table class='tensor-table'>
<tr><th></th><th>[0]</th><th>[1]</th></tr>
<tr><th>[0]</th><td>0.0000</td><td>0.0000</td></tr>
<tr><th>[1]</th><td>0.0000</td><td>-0.0035</td></tr>
</table>
    <h3>General Gradient Formula</h3>
    <p>\(\frac{\partial L}{\partial x_{n,c,h_{in},w_{in}}} = \begin{cases} \frac{\partial L}{\partial y_{n,c,h_{out},w_{out}}} & \text{if } x_{n,c,h_{in},w_{in}} \text{ is max in pool} \\ 0 & \text{otherwise} \end{cases}\)</p>
    </div>
    </div>

<div class='layer-summary'><h2>Layer: conv2 (Conv2d)</h2>

<p><strong>Input Shape</strong>: torch.Size([1, 2, 4, 4])</p>
<p><strong>Output Shape</strong>: torch.Size([1, 4, 4, 4])</p>

<h3>Forward Pass</h3>

<p><strong>General Formula</strong>: \(y_{n,c_{out},h_{out},w_{out}} = \sum_{c_{in},k_h,k_w} x_{n,c_{in},h_{in}+k_h,w_{in}+k_w} \cdot w_{c_{out},c_{in},k_h,k_w} + b_{c_{out}}\)</p>


    <div class="computation-details">
    
    <h3>Computing output at position: (batch=0, out_channel=0, y=1, x=1)</h3>
    
    <h4>Filter Weights (Shape: (2, 3, 3))</h4>
    <h5>Channel 0</h5>
<table class='tensor-table'>
<tr><th></th><th>[0]</th><th>[1]</th><th>[2]</th></tr>
<tr><th>[0]</th><td>0.5000</td><td>0.1862</td><td>0.0398</td></tr>
<tr><th>[1]</th><td>-0.0827</td><td>-0.2315</td><td>-0.1452</td></tr>
<tr><th>[2]</th><td>-0.0208</td><td>0.2278</td><td>-0.0299</td></tr>
</table>
<h5>Channel 1</h5>
<table class='tensor-table'>
<tr><th></th><th>[0]</th><th>[1]</th><th>[2]</th></tr>
<tr><th>[0]</th><td>0.3000</td><td>-0.0553</td><td>0.0391</td></tr>
<tr><th>[1]</th><td>0.2137</td><td>-0.2156</td><td>0.0396</td></tr>
<tr><th>[2]</th><td>0.1689</td><td>0.0722</td><td>0.1024</td></tr>
</table>

    
    <h4>Input Receptive Field (Shape: (3, 3, 2))</h4>
    <h5>Channel 0</h5>
<table class='tensor-table'>
<tr><th></th><th>[0]</th><th>[1]</th></tr>
<tr><th>[0]</th><td>0.5800</td><td>0.3629</td></tr>
<tr><th>[1]</th><td>1.3000</td><td>0.9214</td></tr>
<tr><th>[2]</th><td>1.3686</td><td>0.9700</td></tr>
</table>
<h5>Channel 1</h5>
<table class='tensor-table'>
<tr><th></th><th>[0]</th><th>[1]</th></tr>
<tr><th>[0]</th><td>1.4657</td><td>0.9514</td></tr>
<tr><th>[1]</th><td>1.8643</td><td>1.2243</td></tr>
<tr><th>[2]</th><td>2.2229</td><td>1.3929</td></tr>
</table>
<h5>Channel 2</h5>
<table class='tensor-table'>
<tr><th></th><th>[0]</th><th>[1]</th></tr>
<tr><th>[0]</th><td>1.2486</td><td>0.9400</td></tr>
<tr><th>[1]</th><td>2.0186</td><td>1.4814</td></tr>
<tr><th>[2]</th><td>1.9471</td><td>1.3814</td></tr>
</table>

    
    <h3>General Formula</h3>
    <p>\(y_{n,c_{out},h_{out},w_{out}} = \sum_{c_{in}} \sum_{k_h=0}^{K_h-1} \sum_{k_w=0}^{K_w-1} x_{n,c_{in},h_{in}+k_h,w_{in}+k_w} \cdot w_{c_{out},c_{in},k_h,k_w} + b_{c_{out}}\)</p>
    
    <h3>Value Substitution</h3>
    <p>\((0.5800 \times 0.5000) + (1.3000 \times 0.1862) + (1.3686 \times 0.0398) + (1.4657 \times -0.0827) + (1.8643 \times -0.2315) + (2.2229 \times -0.1452) + (1.2486 \times -0.0208) + (2.0186 \times 0.2278) + (1.9471 \times -0.0299) + (0.3629 \times 0.3000) + (0.9214 \times -0.0553) + (0.9700 \times 0.0391) + (0.9514 \times 0.2137) + (1.2243 \times -0.2156) + (1.3929 \times 0.0396) + (0.9400 \times 0.1689) + (1.4814 \times 0.0722) + (1.3814 \times 0.1024) + 0.0500\)</p>
    
    <h3>Computation Result</h3>
    <p>Calculated value: 0.634222</p>
    <p>Actual output value: 0.634222</p>
    
</div><h3>Backward Pass</h3>

<p><strong>General Formula</strong>: \(\frac{\partial L}{\partial x_{n,c_{in},h_{in},w_{in}}} = \sum_{c_{out},k_h,k_w} \frac{\partial L}{\partial y_{n,c_{out},h_{out},w_{out}}} \cdot w_{c_{out},c_{in},k_h,k_w}\)</p>


    <div class="backward-computation">
    <h3>Backward Computation for Conv2d at Position: (batch=0, out_channel=0, y=1, x=1)</h3>
    
    <h4>Gradient Output Value</h4>
    <p>-0.005189</p>
    
    <h4>Filter Weights</h4>
    <h5>Channel 0</h5>
<table class='tensor-table'>
<tr><th></th><th>[0]</th><th>[1]</th><th>[2]</th></tr>
<tr><th>[0]</th><td>0.5000</td><td>0.1862</td><td>0.0398</td></tr>
<tr><th>[1]</th><td>-0.0827</td><td>-0.2315</td><td>-0.1452</td></tr>
<tr><th>[2]</th><td>-0.0208</td><td>0.2278</td><td>-0.0299</td></tr>
</table>
<h5>Channel 1</h5>
<table class='tensor-table'>
<tr><th></th><th>[0]</th><th>[1]</th><th>[2]</th></tr>
<tr><th>[0]</th><td>0.3000</td><td>-0.0553</td><td>0.0391</td></tr>
<tr><th>[1]</th><td>0.2137</td><td>-0.2156</td><td>0.0396</td></tr>
<tr><th>[2]</th><td>0.1689</td><td>0.0722</td><td>0.1024</td></tr>
</table>

    
    <h4>Gradient Propagation to Input</h4>
    <table class="tensor-table">
    <tr><th>Position (y, x, c)</th><th>Formula</th><th>Gradient Value</th></tr>
    <tr><td>(0, 0, 0)</td><td>\((-0.0052 \times 0.5000)\)</td><td>-0.002595</td></tr>
<tr><td>(0, 1, 0)</td><td>\((-0.0052 \times 0.1862)\)</td><td>-0.000966</td></tr>
<tr><td>(0, 2, 0)</td><td>\((-0.0052 \times 0.0398)\)</td><td>-0.000207</td></tr>
<tr><td>(1, 0, 0)</td><td>\((-0.0052 \times -0.0827)\)</td><td>0.000429</td></tr>
<tr><td>(1, 1, 0)</td><td>\((-0.0052 \times -0.2315)\)</td><td>0.001201</td></tr>
<tr><td>(1, 2, 0)</td><td>\((-0.0052 \times -0.1452)\)</td><td>0.000754</td></tr>
<tr><td>(2, 0, 0)</td><td>\((-0.0052 \times -0.0208)\)</td><td>0.000108</td></tr>
<tr><td>(2, 1, 0)</td><td>\((-0.0052 \times 0.2278)\)</td><td>-0.001182</td></tr>
<tr><td>(2, 2, 0)</td><td>\((-0.0052 \times -0.0299)\)</td><td>0.000155</td></tr>
<tr><td>(0, 0, 1)</td><td>\((-0.0052 \times 0.3000)\)</td><td>-0.001557</td></tr>
<tr><td>(0, 1, 1)</td><td>\((-0.0052 \times -0.0553)\)</td><td>0.000287</td></tr>
<tr><td>(0, 2, 1)</td><td>\((-0.0052 \times 0.0391)\)</td><td>-0.000203</td></tr>
<tr><td>(1, 0, 1)</td><td>\((-0.0052 \times 0.2137)\)</td><td>-0.001109</td></tr>
<tr><td>(1, 1, 1)</td><td>\((-0.0052 \times -0.2156)\)</td><td>0.001119</td></tr>
<tr><td>(1, 2, 1)</td><td>\((-0.0052 \times 0.0396)\)</td><td>-0.000206</td></tr>
<tr><td>(2, 0, 1)</td><td>\((-0.0052 \times 0.1689)\)</td><td>-0.000876</td></tr>
<tr><td>(2, 1, 1)</td><td>\((-0.0052 \times 0.0722)\)</td><td>-0.000374</td></tr>
<tr><td>(2, 2, 1)</td><td>\((-0.0052 \times 0.1024)\)</td><td>-0.000531</td></tr>

    </table>
    
    <h4>Weight Gradient Computation</h4>
    <table class="tensor-table">
    <tr><th>Weight Position (out_c, in_c, ky, kx)</th><th>Formula</th><th>Gradient Value</th></tr>
    <tr><td>(0, 0, 0, 0)</td><td>\((-0.0052 \times 0.5800)\)</td><td>-0.003010</td></tr>
<tr><td>(0, 0, 0, 1)</td><td>\((-0.0052 \times 1.3000)\)</td><td>-0.006746</td></tr>
<tr><td>(0, 0, 0, 2)</td><td>\((-0.0052 \times 1.3686)\)</td><td>-0.007102</td></tr>
<tr><td>(0, 0, 1, 0)</td><td>\((-0.0052 \times 1.4657)\)</td><td>-0.007606</td></tr>
<tr><td>(0, 0, 1, 1)</td><td>\((-0.0052 \times 1.8643)\)</td><td>-0.009674</td></tr>
<tr><td>(0, 0, 1, 2)</td><td>\((-0.0052 \times 2.2229)\)</td><td>-0.011535</td></tr>
<tr><td>(0, 0, 2, 0)</td><td>\((-0.0052 \times 1.2486)\)</td><td>-0.006479</td></tr>
<tr><td>(0, 0, 2, 1)</td><td>\((-0.0052 \times 2.0186)\)</td><td>-0.010475</td></tr>
<tr><td>(0, 0, 2, 2)</td><td>\((-0.0052 \times 1.9471)\)</td><td>-0.010104</td></tr>
<tr><td>(0, 1, 0, 0)</td><td>\((-0.0052 \times 0.3629)\)</td><td>-0.001883</td></tr>
<tr><td>(0, 1, 0, 1)</td><td>\((-0.0052 \times 0.9214)\)</td><td>-0.004781</td></tr>
<tr><td>(0, 1, 0, 2)</td><td>\((-0.0052 \times 0.9700)\)</td><td>-0.005034</td></tr>
<tr><td>(0, 1, 1, 0)</td><td>\((-0.0052 \times 0.9514)\)</td><td>-0.004937</td></tr>
<tr><td>(0, 1, 1, 1)</td><td>\((-0.0052 \times 1.2243)\)</td><td>-0.006353</td></tr>
<tr><td>(0, 1, 1, 2)</td><td>\((-0.0052 \times 1.3929)\)</td><td>-0.007228</td></tr>
<tr><td>(0, 1, 2, 0)</td><td>\((-0.0052 \times 0.9400)\)</td><td>-0.004878</td></tr>
<tr><td>(0, 1, 2, 1)</td><td>\((-0.0052 \times 1.4814)\)</td><td>-0.007687</td></tr>
<tr><td>(0, 1, 2, 2)</td><td>\((-0.0052 \times 1.3814)\)</td><td>-0.007169</td></tr>

    </table>
    
    <h4>Bias Gradient Computation</h4>
    <p>Bias Gradient for output channel 0: -0.005189</p>
    
    <h3>General Gradient Formulas</h3>
    
    <h4>Input Gradient Formula</h4>
    <p>\(\frac{\partial L}{\partial x_{n,c_{in},h_{in},w_{in}}} = \sum_{c_{out}} \sum_{k_h=0}^{K_h-1} \sum_{k_w=0}^{K_w-1} \frac{\partial L}{\partial y_{n,c_{out},h_{out},w_{out}}} \cdot w_{c_{out},c_{in},k_h,k_w}\)</p>
    
    <h4>Weight Gradient Formula</h4>
    <p>\(\frac{\partial L}{\partial w_{c_{out},c_{in},k_h,k_w}} = \sum_{n} \sum_{h_{out},w_{out}} \frac{\partial L}{\partial y_{n,c_{out},h_{out},w_{out}}} \cdot x_{n,c_{in},h_{in}+k_h,w_{in}+k_w}\)</p>
    
    <h4>Bias Gradient Formula</h4>
    <p>\(\frac{\partial L}{\partial b_{c_{out}}} = \sum_{n} \sum_{h_{out},w_{out}} \frac{\partial L}{\partial y_{n,c_{out},h_{out},w_{out}}}\)</p>
    </div>
    </div>

<div class='layer-summary'><h2>Layer: pool2 (AvgPool2d)</h2>

<p><strong>Input Shape</strong>: torch.Size([1, 4, 4, 4])</p>
<p><strong>Output Shape</strong>: torch.Size([1, 4, 2, 2])</p>

<h3>Forward Pass</h3>

<p><strong>General Formula</strong>: \(y_{n,c,h_{out},w_{out}} = \frac{1}{k_h \cdot k_w} \sum_{i=0}^{k_h-1} \sum_{j=0}^{k_w-1} x_{n,c,h_{in}+i,w_{in}+j}\)</p>


    <div class="computation-details">
    
    <h3>Computing output at position: (batch=0, channel=0, y=1, x=1)</h3>
    
    <h4>Input Receptive Field (Shape: (2, 2))</h4>
    <table class='tensor-table'>
<tr><th></th><th>[0]</th><th>[1]</th></tr>
<tr><th>[0]</th><td>1.7873</td><td>1.9932</td></tr>
<tr><th>[1]</th><td>0.9956</td><td>1.1012</td></tr>
</table>
    <p>Kernel size: (2, 2)</p>
    
    <h3>General Formula</h3>
    <p>\(y_{n,c,h_{out},w_{out}} = \frac{1}{k_h \cdot k_w} \sum_{i=0}^{k_h-1} \sum_{j=0}^{k_w-1} x_{n,c,h_{in}+i,w_{in}+j}\)</p>
    
    <h3>Value Substitution</h3>
    <p>\(\frac1{2 \times 2} \times (1.7873 + 1.9932 + 0.9956 + 1.1012)\)</p>
    
    <h3>Computation Result</h3>
    <p>Calculated average value: 1.469322</p>
    <p>Actual output value: 1.469322</p>
    
</div><h3>Backward Pass</h3>

<p><strong>General Formula</strong>: \(\frac{\partial L}{\partial x_{n,c,h_{in},w_{in}}} = \frac{1}{k_h \cdot k_w} \sum_{h_{out},w_{out}} \frac{\partial L}{\partial y_{n,c,h_{out},w_{out}}}\)</p>


    <div class="backward-computation">
    <h3>Backward Computation for AvgPool2d at Position: (batch=0, channel=0, y=1, x=1)</h3>
    
    <h4>Gradient Output Value</h4>
    <p>0.007160</p>
    
    <p>Pool Size: 4 (kernel: (2, 2))</p>
    <p>Distributed Gradient: 0.001790 (0.007160 / 4)</p>
    
    <h4>Gradient Propagation to Input</h4>
    <table class="tensor-table">
    <tr><th>Position (y, x)</th><th>Formula</th><th>Gradient Value</th></tr>
    <tr><td>(2, 2)</td><td>\(0.0072 / 4\)</td><td>0.001790</td></tr>
<tr><td>(2, 3)</td><td>\(0.0072 / 4\)</td><td>0.001790</td></tr>
<tr><td>(3, 2)</td><td>\(0.0072 / 4\)</td><td>0.001790</td></tr>
<tr><td>(3, 3)</td><td>\(0.0072 / 4\)</td><td>0.001790</td></tr>

    </table>
    
    <h4>Gradient Map (Receptive Field)</h4>
    <table class='tensor-table'>
<tr><th></th><th>[0]</th><th>[1]</th></tr>
<tr><th>[0]</th><td>0.0018</td><td>0.0018</td></tr>
<tr><th>[1]</th><td>0.0018</td><td>0.0018</td></tr>
</table>
    <h3>General Gradient Formula</h3>
    <p>\(\frac{\partial L}{\partial x_{n,c,h_{in},w_{in}}} = \frac{1}{k_h \cdot k_w} \frac{\partial L}{\partial y_{n,c,h_{out},w_{out}}}\)</p>
    </div>
    </div>

<div class='layer-summary'><h2>Layer: conv3 (Conv2d)</h2>

<p><strong>Input Shape</strong>: torch.Size([1, 4, 2, 2])</p>
<p><strong>Output Shape</strong>: torch.Size([1, 2, 2, 2])</p>

<h3>Forward Pass</h3>

<p><strong>General Formula</strong>: \(y_{n,c_{out},h_{out},w_{out}} = \sum_{c_{in},k_h,k_w} x_{n,c_{in},h_{in}+k_h,w_{in}+k_w} \cdot w_{c_{out},c_{in},k_h,k_w} + b_{c_{out}}\)</p>


    <div class="computation-details">
    
    <h3>Computing output at position: (batch=0, out_channel=0, y=1, x=1)</h3>
    
    <h4>Filter Weights (Shape: (4, 1, 1))</h4>
    <h5>Channel 0</h5>
<table class='tensor-table'>
<tr><th></th><th>[0]</th></tr>
<tr><th>[0]</th><td>0.1034</td></tr>
</table>
<h5>Channel 1</h5>
<table class='tensor-table'>
<tr><th></th><th>[0]</th></tr>
<tr><th>[0]</th><td>0.3807</td></tr>
</table>
<h5>Channel 2</h5>
<table class='tensor-table'>
<tr><th></th><th>[0]</th></tr>
<tr><th>[0]</th><td>0.1710</td></tr>
</table>
<h5>Channel 3</h5>
<table class='tensor-table'>
<tr><th></th><th>[0]</th></tr>
<tr><th>[0]</th><td>0.4088</td></tr>
</table>

    
    <h4>Input Receptive Field (Shape: (1, 1, 4))</h4>
    <h5>Channel 0</h5>
<table class='tensor-table'>
<tr><th></th><th>[0]</th><th>[1]</th><th>[2]</th><th>[3]</th></tr>
<tr><th>[0]</th><td>1.4693</td><td>0.8798</td><td>0.0000</td><td>0.0000</td></tr>
</table>

    
    <h3>General Formula</h3>
    <p>\(y_{n,c_{out},h_{out},w_{out}} = \sum_{c_{in}} \sum_{k_h=0}^{K_h-1} \sum_{k_w=0}^{K_w-1} x_{n,c_{in},h_{in}+k_h,w_{in}+k_w} \cdot w_{c_{out},c_{in},k_h,k_w} + b_{c_{out}}\)</p>
    
    <h3>Value Substitution</h3>
    <p>\((1.4693 \times 0.1034) + (0.8798 \times 0.3807) + (0.0000 \times 0.1710) + (0.0000 \times 0.4088) + 0.2806\)</p>
    
    <h3>Computation Result</h3>
    <p>Calculated value: 0.767475</p>
    <p>Actual output value: 0.767475</p>
    
</div><h3>Backward Pass</h3>

<p><strong>General Formula</strong>: \(\frac{\partial L}{\partial x_{n,c_{in},h_{in},w_{in}}} = \sum_{c_{out},k_h,k_w} \frac{\partial L}{\partial y_{n,c_{out},h_{out},w_{out}}} \cdot w_{c_{out},c_{in},k_h,k_w}\)</p>


    <div class="backward-computation">
    <h3>Backward Computation for Conv2d at Position: (batch=0, out_channel=0, y=1, x=1)</h3>
    
    <h4>Gradient Output Value</h4>
    <p>0.069219</p>
    
    <h4>Filter Weights</h4>
    <h5>Channel 0</h5>
<table class='tensor-table'>
<tr><th></th><th>[0]</th></tr>
<tr><th>[0]</th><td>0.1034</td></tr>
</table>
<h5>Channel 1</h5>
<table class='tensor-table'>
<tr><th></th><th>[0]</th></tr>
<tr><th>[0]</th><td>0.3807</td></tr>
</table>
<h5>Channel 2</h5>
<table class='tensor-table'>
<tr><th></th><th>[0]</th></tr>
<tr><th>[0]</th><td>0.1710</td></tr>
</table>
<h5>Channel 3</h5>
<table class='tensor-table'>
<tr><th></th><th>[0]</th></tr>
<tr><th>[0]</th><td>0.4088</td></tr>
</table>

    
    <h4>Gradient Propagation to Input</h4>
    <table class="tensor-table">
    <tr><th>Position (y, x, c)</th><th>Formula</th><th>Gradient Value</th></tr>
    <tr><td>(1, 1, 0)</td><td>\((0.0692 \times 0.1034)\)</td><td>0.007160</td></tr>
<tr><td>(1, 1, 1)</td><td>\((0.0692 \times 0.3807)\)</td><td>0.026349</td></tr>
<tr><td>(1, 1, 2)</td><td>\((0.0692 \times 0.1710)\)</td><td>0.011834</td></tr>
<tr><td>(1, 1, 3)</td><td>\((0.0692 \times 0.4088)\)</td><td>0.028297</td></tr>

    </table>
    
    <h4>Weight Gradient Computation</h4>
    <table class="tensor-table">
    <tr><th>Weight Position (out_c, in_c, ky, kx)</th><th>Formula</th><th>Gradient Value</th></tr>
    <tr><td>(0, 0, 0, 0)</td><td>\((0.0692 \times 1.4693)\)</td><td>0.101706</td></tr>
<tr><td>(0, 1, 0, 0)</td><td>\((0.0692 \times 0.8798)\)</td><td>0.060898</td></tr>
<tr><td>(0, 2, 0, 0)</td><td>\((0.0692 \times 0.0000)\)</td><td>0.000000</td></tr>
<tr><td>(0, 3, 0, 0)</td><td>\((0.0692 \times 0.0000)\)</td><td>0.000000</td></tr>

    </table>
    
    <h4>Bias Gradient Computation</h4>
    <p>Bias Gradient for output channel 0: 0.069219</p>
    
    <h3>General Gradient Formulas</h3>
    
    <h4>Input Gradient Formula</h4>
    <p>\(\frac{\partial L}{\partial x_{n,c_{in},h_{in},w_{in}}} = \sum_{c_{out}} \sum_{k_h=0}^{K_h-1} \sum_{k_w=0}^{K_w-1} \frac{\partial L}{\partial y_{n,c_{out},h_{out},w_{out}}} \cdot w_{c_{out},c_{in},k_h,k_w}\)</p>
    
    <h4>Weight Gradient Formula</h4>
    <p>\(\frac{\partial L}{\partial w_{c_{out},c_{in},k_h,k_w}} = \sum_{n} \sum_{h_{out},w_{out}} \frac{\partial L}{\partial y_{n,c_{out},h_{out},w_{out}}} \cdot x_{n,c_{in},h_{in}+k_h,w_{in}+k_w}\)</p>
    
    <h4>Bias Gradient Formula</h4>
    <p>\(\frac{\partial L}{\partial b_{c_{out}}} = \sum_{n} \sum_{h_{out},w_{out}} \frac{\partial L}{\partial y_{n,c_{out},h_{out},w_{out}}}\)</p>
    </div>
    </div>

<div class='layer-summary'><h2>Layer: conv_out (Conv2d)</h2>

<p><strong>Input Shape</strong>: torch.Size([1, 2, 2, 2])</p>
<p><strong>Output Shape</strong>: torch.Size([1, 2, 1, 1])</p>

<h3>Forward Pass</h3>

<p><strong>General Formula</strong>: \(y_{n,c_{out},h_{out},w_{out}} = \sum_{c_{in},k_h,k_w} x_{n,c_{in},h_{in}+k_h,w_{in}+k_w} \cdot w_{c_{out},c_{in},k_h,k_w} + b_{c_{out}}\)</p>


    <div class="computation-details">
    
    <h3>Computing output at position: (batch=0, out_channel=0, y=0, x=0)</h3>
    
    <h4>Filter Weights (Shape: (2, 2, 2))</h4>
    <h5>Channel 0</h5>
<table class='tensor-table'>
<tr><th></th><th>[0]</th><th>[1]</th></tr>
<tr><th>[0]</th><td>0.3106</td><td>-0.1264</td></tr>
<tr><th>[1]</th><td>-0.2203</td><td>-0.0560</td></tr>
</table>
<h5>Channel 1</h5>
<table class='tensor-table'>
<tr><th></th><th>[0]</th><th>[1]</th></tr>
<tr><th>[0]</th><td>0.1043</td><td>0.2032</td></tr>
<tr><th>[1]</th><td>-0.0275</td><td>0.1508</td></tr>
</table>

    
    <h4>Input Receptive Field (Shape: (2, 2, 2))</h4>
    <h5>Channel 0</h5>
<table class='tensor-table'>
<tr><th></th><th>[0]</th><th>[1]</th></tr>
<tr><th>[0]</th><td>0.6659</td><td>0.0000</td></tr>
<tr><th>[1]</th><td>0.5708</td><td>0.0000</td></tr>
</table>
<h5>Channel 1</h5>
<table class='tensor-table'>
<tr><th></th><th>[0]</th><th>[1]</th></tr>
<tr><th>[0]</th><td>0.7313</td><td>0.0000</td></tr>
<tr><th>[1]</th><td>0.7675</td><td>0.0000</td></tr>
</table>

    
    <h3>General Formula</h3>
    <p>\(y_{n,c_{out},h_{out},w_{out}} = \sum_{c_{in}} \sum_{k_h=0}^{K_h-1} \sum_{k_w=0}^{K_w-1} x_{n,c_{in},h_{in}+k_h,w_{in}+k_w} \cdot w_{c_{out},c_{in},k_h,k_w} + b_{c_{out}}\)</p>
    
    <h3>Value Substitution</h3>
    <p>\((0.6659 \times 0.3106) + (0.5708 \times -0.1264) + (0.7313 \times -0.2203) + (0.7675 \times -0.0560) + (0.0000 \times 0.1043) + (0.0000 \times 0.2032) + (0.0000 \times -0.0275) + (0.0000 \times 0.1508) + -0.3441\)</p>
    
    <h3>Computation Result</h3>
    <p>Calculated value: -0.413463</p>
    <p>Actual output value: -0.413463</p>
    
</div><h3>Backward Pass</h3>

<p><strong>General Formula</strong>: \(\frac{\partial L}{\partial x_{n,c_{in},h_{in},w_{in}}} = \sum_{c_{out},k_h,k_w} \frac{\partial L}{\partial y_{n,c_{out},h_{out},w_{out}}} \cdot w_{c_{out},c_{in},k_h,k_w}\)</p>


    <div class="backward-computation">
    <h3>Backward Computation for Conv2d at Position: (batch=0, out_channel=0, y=0, x=0)</h3>
    
    <h4>Gradient Output Value</h4>
    <p>-0.750153</p>
    
    <h4>Filter Weights</h4>
    <h5>Channel 0</h5>
<table class='tensor-table'>
<tr><th></th><th>[0]</th><th>[1]</th></tr>
<tr><th>[0]</th><td>0.3106</td><td>-0.1264</td></tr>
<tr><th>[1]</th><td>-0.2203</td><td>-0.0560</td></tr>
</table>
<h5>Channel 1</h5>
<table class='tensor-table'>
<tr><th></th><th>[0]</th><th>[1]</th></tr>
<tr><th>[0]</th><td>0.1043</td><td>0.2032</td></tr>
<tr><th>[1]</th><td>-0.0275</td><td>0.1508</td></tr>
</table>

    
    <h4>Gradient Propagation to Input</h4>
    <table class="tensor-table">
    <tr><th>Position (y, x, c)</th><th>Formula</th><th>Gradient Value</th></tr>
    <tr><td>(0, 0, 0)</td><td>\((-0.7502 \times 0.3106)\)</td><td>-0.233020</td></tr>
<tr><td>(0, 1, 0)</td><td>\((-0.7502 \times -0.1264)\)</td><td>0.094843</td></tr>
<tr><td>(1, 0, 0)</td><td>\((-0.7502 \times -0.2203)\)</td><td>0.165264</td></tr>
<tr><td>(1, 1, 0)</td><td>\((-0.7502 \times -0.0560)\)</td><td>0.042000</td></tr>
<tr><td>(0, 0, 1)</td><td>\((-0.7502 \times 0.1043)\)</td><td>-0.078245</td></tr>
<tr><td>(0, 1, 1)</td><td>\((-0.7502 \times 0.2032)\)</td><td>-0.152427</td></tr>
<tr><td>(1, 0, 1)</td><td>\((-0.7502 \times -0.0275)\)</td><td>0.020658</td></tr>
<tr><td>(1, 1, 1)</td><td>\((-0.7502 \times 0.1508)\)</td><td>-0.113148</td></tr>

    </table>
    
    <h4>Weight Gradient Computation</h4>
    <table class="tensor-table">
    <tr><th>Weight Position (out_c, in_c, ky, kx)</th><th>Formula</th><th>Gradient Value</th></tr>
    <tr><td>(0, 0, 0, 0)</td><td>\((-0.7502 \times 0.6659)\)</td><td>-0.499529</td></tr>
<tr><td>(0, 0, 0, 1)</td><td>\((-0.7502 \times 0.5708)\)</td><td>-0.428204</td></tr>
<tr><td>(0, 0, 1, 0)</td><td>\((-0.7502 \times 0.7313)\)</td><td>-0.548584</td></tr>
<tr><td>(0, 0, 1, 1)</td><td>\((-0.7502 \times 0.7675)\)</td><td>-0.575724</td></tr>
<tr><td>(0, 1, 0, 0)</td><td>\((-0.7502 \times 0.0000)\)</td><td>-0.000000</td></tr>
<tr><td>(0, 1, 0, 1)</td><td>\((-0.7502 \times 0.0000)\)</td><td>-0.000000</td></tr>
<tr><td>(0, 1, 1, 0)</td><td>\((-0.7502 \times 0.0000)\)</td><td>-0.000000</td></tr>
<tr><td>(0, 1, 1, 1)</td><td>\((-0.7502 \times 0.0000)\)</td><td>-0.000000</td></tr>

    </table>
    
    <h4>Bias Gradient Computation</h4>
    <p>Bias Gradient for output channel 0: -0.750153</p>
    
    <h3>General Gradient Formulas</h3>
    
    <h4>Input Gradient Formula</h4>
    <p>\(\frac{\partial L}{\partial x_{n,c_{in},h_{in},w_{in}}} = \sum_{c_{out}} \sum_{k_h=0}^{K_h-1} \sum_{k_w=0}^{K_w-1} \frac{\partial L}{\partial y_{n,c_{out},h_{out},w_{out}}} \cdot w_{c_{out},c_{in},k_h,k_w}\)</p>
    
    <h4>Weight Gradient Formula</h4>
    <p>\(\frac{\partial L}{\partial w_{c_{out},c_{in},k_h,k_w}} = \sum_{n} \sum_{h_{out},w_{out}} \frac{\partial L}{\partial y_{n,c_{out},h_{out},w_{out}}} \cdot x_{n,c_{in},h_{in}+k_h,w_{in}+k_w}\)</p>
    
    <h4>Bias Gradient Formula</h4>
    <p>\(\frac{\partial L}{\partial b_{c_{out}}} = \sum_{n} \sum_{h_{out},w_{out}} \frac{\partial L}{\partial y_{n,c_{out},h_{out},w_{out}}}\)</p>
    </div>
    </div></body>
</html>