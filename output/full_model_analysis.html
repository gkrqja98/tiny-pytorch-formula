<!DOCTYPE html>
<html>
<head>
    <title>PyTorch Model Analysis</title>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <!-- KaTeX for faster rendering -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css" integrity="sha384-GvrOXuhMATgEsSwCs4smul74iXGOixntILdUW9XmUC6+HX0sLNAK3q71HotJqlAn" crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js" integrity="sha384-cpW21h6RZv/phavutF+AuVYrr+dA8xD9zs6FwLpaCct6O9ctzYFfFr4dgmgccOTx" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            renderMathInElement(document.body, {
                delimiters: [
                    {left: "$$", right: "$$", display: true},
                    {left: "$", right: "$", display: false},
                    {left: "\\[", right: "\\]", display: true},
                    {left: "\\(", right: "\\)", display: false}
                ],
                throwOnError: false
            });
        });
    </script>
    <style>
        body { font-family: Arial, sans-serif; margin: 20px; line-height: 1.6; }
        h1 { color: #2c3e50; }
        h2 { color: #3498db; border-bottom: 1px solid #3498db; padding-bottom: 5px; }
        h3 { color: #2980b9; }
        h4 { color: #27ae60; }
        .layer-summary { margin-bottom: 30px; padding: 15px; border: 1px solid #ddd; border-radius: 5px; box-shadow: 0 2px 4px rgba(0,0,0,0.1); }
        .forward-pass { border-left: 5px solid #2ecc71; padding-left: 15px; }
        .backward-pass { border-left: 5px solid #e74c3c; padding-left: 15px; }
        .tensor-table { border-collapse: collapse; margin: 10px 0; width: auto; }
        .tensor-table th, .tensor-table td { border: 1px solid #ddd; padding: 8px; text-align: right; }
        .tensor-table th { background-color: #f2f2f2; }
        .tensor-header { background-color: #e3f2fd; }
        .channel-separator { border-bottom: 2px solid #3498db; }
        .batch-separator { border-bottom: 3px double #e74c3c; }
        .formula { background-color: #f9f9f9; padding: 10px; border-radius: 5px; overflow-x: auto; margin: 15px 0; }
        .general-formula { font-weight: bold; }
        .math-container { overflow-x: auto; padding: 10px 0; }
        .collapsible { 
            background-color: #f1f1f1;
            cursor: pointer;
            padding: 10px;
            width: 100%;
            border: none;
            text-align: left;
            outline: none;
            font-size: 15px;
            border-radius: 5px;
        }
        .active, .collapsible:hover { background-color: #e0e0e0; }
        .content { 
            padding: 0 18px;
            max-height: 0;
            overflow: hidden;
            transition: max-height 0.2s ease-out;
            background-color: #ffffff;
            border-radius: 0 0 5px 5px;
        }
        /* Improved responsive design */
        @media screen and (max-width: 768px) {
            body { margin: 10px; }
            .tensor-table { width: 100%; overflow-x: auto; display: block; }
            .math-container { overflow-x: auto; max-width: 100%; }
            .tab button { padding: 10px 12px; font-size: 15px; }
        }
        .visualization-container { 
            display: flex;
            flex-wrap: wrap;
            gap: 15px;
            justify-content: center;
            margin: 20px 0;
        }
        .visualization-item {
            max-width: 100%;
            border: 1px solid #ddd;
            border-radius: 5px;
            overflow: hidden;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }
        .visualization-item img {
            max-width: 100%;
            display: block;
        }
        .visualization-caption {
            padding: 10px;
            background-color: #f9f9f9;
            text-align: center;
            font-weight: bold;
        }
        .tab {
            overflow: hidden;
            border: 1px solid #ccc;
            background-color: #f1f1f1;
            border-radius: 5px 5px 0 0;
        }
        .tab button {
            background-color: inherit;
            float: left;
            border: none;
            outline: none;
            cursor: pointer;
            padding: 14px 16px;
            transition: 0.3s;
            font-size: 17px;
        }
        .tab button:hover { background-color: #ddd; }
        .tab button.active { background-color: #ccc; }
        .tabcontent {
            display: none;
            padding: 6px 12px;
            border: 1px solid #ccc;
            border-top: none;
            border-radius: 0 0 5px 5px;
        }
    </style>
    <script>
    document.addEventListener('DOMContentLoaded', function() {
        // Collapsible sections
        var coll = document.getElementsByClassName("collapsible");
        for (var i = 0; i < coll.length; i++) {
            coll[i].addEventListener("click", function() {
                this.classList.toggle("active");
                var content = this.nextElementSibling;
                if (content.style.maxHeight) {
                    content.style.maxHeight = null;
                } else {
                    content.style.maxHeight = content.scrollHeight + "px";
                }
            });
        }
        
        // Tabs
        function openTab(evt, tabName) {
            var i, tabcontent, tablinks;
            tabcontent = document.getElementsByClassName("tabcontent");
            for (i = 0; i < tabcontent.length; i++) {
                tabcontent[i].style.display = "none";
            }
            tablinks = document.getElementsByClassName("tablinks");
            for (i = 0; i < tablinks.length; i++) {
                tablinks[i].className = tablinks[i].className.replace(" active", "");
            }
            document.getElementById(tabName).style.display = "block";
            evt.currentTarget.className += " active";
            
            // Re-render KaTeX in the newly displayed tab
            if (typeof renderMathInElement === 'function') {
                renderMathInElement(document.getElementById(tabName), {
                    delimiters: [
                        {left: "$$", right: "$$", display: true},
                        {left: "$", right: "$", display: false},
                        {left: "\\[", right: "\\]", display: true},
                        {left: "\\(", right: "\\)", display: false}
                    ],
                    throwOnError: false
                });
            }
        }
        
        // Open the first tab by default
        if (document.getElementsByClassName("tablinks").length > 0) {
            document.getElementsByClassName("tablinks")[0].click();
        }
        
        // Expose the openTab function globally
        window.openTab = openTab;
    });
    </script>
</head>
<body>
    <h1>Complete Forward and Backward Analysis</h1>
<p><strong>Input Shape:</strong> torch.Size([1, 1, 8, 8])</p>
<p><strong>Output Shape:</strong> torch.Size([1, 2])</p>
<p><strong>Loss Value:</strong> 0.030797</p>
<div class='layer-summary'><h2>Layer: conv1 (Conv2d)</h2>

<p><strong>Input Shape</strong>: torch.Size([1, 1, 8, 8])</p>
<p><strong>Output Shape</strong>: torch.Size([1, 2, 8, 8])</p>

<h3>Forward Pass</h3>

<p><strong>General Formula</strong>: \(y_{n,c_{out},h_{out},w_{out}} = \sum_{c_{in},k_h,k_w} x_{n,c_{in},h_{in}+k_h,w_{in}+k_w} \cdot w_{c_{out},c_{in},k_h,k_w} + b_{c_{out}}\)</p>


    <div class="computation-details">
    
    <h3>Computing output at position: batch = 0, out_channel = 0, y = 1, x = 1</h3>
    
    <h4>Filter Weights (Shape: (1, 3, 3))</h4>
    <h5>Channel 0</h5>
<table class='tensor-table'>
<tr><th></th><th>[0]</th><th>[1]</th><th>[2]</th></tr>
<tr><th>[0]</th><td>0.3000</td><td>-0.5000</td><td>0.2000</td></tr>
<tr><th>[1]</th><td>0.7000</td><td>0.4000</td><td>-0.1000</td></tr>
<tr><th>[2]</th><td>-0.3000</td><td>0.8000</td><td>0.5000</td></tr>
</table>

    
    <h4>Input Receptive Field (Shape: (3, 3, 1))</h4>
    <h5>Channel 0</h5>
<table class='tensor-table'>
<tr><th></th><th>[0]</th></tr>
<tr><th>[0]</th><td>0.2000</td></tr>
<tr><th>[1]</th><td>-0.0286</td></tr>
<tr><th>[2]</th><td>0.3429</td></tr>
</table>
<h5>Channel 1</h5>
<table class='tensor-table'>
<tr><th></th><th>[0]</th></tr>
<tr><th>[0]</th><td>-0.0286</td></tr>
<tr><th>[1]</th><td>0.3429</td></tr>
<tr><th>[2]</th><td>0.1143</td></tr>
</table>
<h5>Channel 2</h5>
<table class='tensor-table'>
<tr><th></th><th>[0]</th></tr>
<tr><th>[0]</th><td>0.3429</td></tr>
<tr><th>[1]</th><td>0.1143</td></tr>
<tr><th>[2]</th><td>0.4857</td></tr>
</table>

    
    <div class="formula">
        <h4>General Formula</h4>
        <div class="math-container">$$y_{n,c_{out},h_{out},w_{out}} = \sum_{c_{in}} \sum_{k_h=0}^{K_h-1} \sum_{k_w=0}^{K_w-1} x_{n,c_{in},h_{in}+k_h,w_{in}+k_w} \cdot w_{c_{out},c_{in},k_h,k_w} + b_{c_{out}}$$</div>
    </div>
    
    <div class="formula">
        <h4>Value Substitution</h4>
        <div class="math-container">$$(0.2000 \times 0.3000) + (-0.0286 \times -0.5000) + (0.3429 \times 0.2000) + (-0.0286 \times 0.7000) + (0.3429 \times 0.4000) + (0.1143 \times -0.1000) + (0.3429 \times -0.3000) + (0.1143 \times 0.8000) + (0.4857 \times 0.5000) + 0.1000$$</div>
    </div>
    
    <h3>Computation Result</h3>
    <p>Calculated value: 0.580000</p>
    <p>Actual output value: 0.580000</p>
    
</div><h3>Backward Pass</h3>

<p><strong>General Formula</strong>: \(\frac{\partial L}{\partial x_{n,c_{in},h_{in},w_{in}}} = \sum_{c_{out},k_h,k_w} \frac{\partial L}{\partial y_{n,c_{out},h_{out},w_{out}}} \cdot w_{c_{out},c_{in},k_h,k_w}\)</p>


    <div class="backward-computation">
    <h3>Backward Computation for Conv2d at Position: batch = 0, out_channel = 0, y = 1, x = 1</h3>
    
    <h4>Gradient Output Value</h4>
    <p>-0.000228</p>
    
    <h4>Filter Weights</h4>
    <h5>Channel 0</h5>
<table class='tensor-table'>
<tr><th></th><th>[0]</th><th>[1]</th><th>[2]</th></tr>
<tr><th>[0]</th><td>0.3000</td><td>-0.5000</td><td>0.2000</td></tr>
<tr><th>[1]</th><td>0.7000</td><td>0.4000</td><td>-0.1000</td></tr>
<tr><th>[2]</th><td>-0.3000</td><td>0.8000</td><td>0.5000</td></tr>
</table>

    
    <h4>Gradient Propagation to Input</h4>
    <table class="tensor-table">
    <tr><th>Position (y, x, c)</th><th>Formula</th><th>Gradient Value</th></tr>
    <tr><td>(0, 0, 0)</td><td>$(-0.0002 \times 0.3000)$</td><td>-0.000068</td></tr>
<tr><td>(0, 1, 0)</td><td>$(-0.0002 \times -0.5000)$</td><td>0.000114</td></tr>
<tr><td>(0, 2, 0)</td><td>$(-0.0002 \times 0.2000)$</td><td>-0.000046</td></tr>
<tr><td>(1, 0, 0)</td><td>$(-0.0002 \times 0.7000)$</td><td>-0.000160</td></tr>
<tr><td>(1, 1, 0)</td><td>$(-0.0002 \times 0.4000)$</td><td>-0.000091</td></tr>
<tr><td>(1, 2, 0)</td><td>$(-0.0002 \times -0.1000)$</td><td>0.000023</td></tr>
<tr><td>(2, 0, 0)</td><td>$(-0.0002 \times -0.3000)$</td><td>0.000068</td></tr>
<tr><td>(2, 1, 0)</td><td>$(-0.0002 \times 0.8000)$</td><td>-0.000183</td></tr>
<tr><td>(2, 2, 0)</td><td>$(-0.0002 \times 0.5000)$</td><td>-0.000114</td></tr>

    </table>
    
    <h4>Weight Gradient Computation</h4>
    <table class="tensor-table">
    <tr><th>Weight Position (out_c, in_c, ky, kx)</th><th>Formula</th><th>Gradient Value</th></tr>
    <tr><td>(0, 0, 0, 0)</td><td>$(-0.0002 \times 0.2000)$</td><td>-0.000046</td></tr>
<tr><td>(0, 0, 0, 1)</td><td>$(-0.0002 \times -0.0286)$</td><td>0.000007</td></tr>
<tr><td>(0, 0, 0, 2)</td><td>$(-0.0002 \times 0.3429)$</td><td>-0.000078</td></tr>
<tr><td>(0, 0, 1, 0)</td><td>$(-0.0002 \times -0.0286)$</td><td>0.000007</td></tr>
<tr><td>(0, 0, 1, 1)</td><td>$(-0.0002 \times 0.3429)$</td><td>-0.000078</td></tr>
<tr><td>(0, 0, 1, 2)</td><td>$(-0.0002 \times 0.1143)$</td><td>-0.000026</td></tr>
<tr><td>(0, 0, 2, 0)</td><td>$(-0.0002 \times 0.3429)$</td><td>-0.000078</td></tr>
<tr><td>(0, 0, 2, 1)</td><td>$(-0.0002 \times 0.1143)$</td><td>-0.000026</td></tr>
<tr><td>(0, 0, 2, 2)</td><td>$(-0.0002 \times 0.4857)$</td><td>-0.000111</td></tr>

    </table>
    
    <h4>Bias Gradient Computation</h4>
    <p>Bias Gradient for output channel 0: -0.000228</p>
    
    <h3>General Gradient Formulas</h3>
    
    <div class="formula">
        <h4>Input Gradient Formula</h4>
        <div class="math-container">$$\frac{\partial L}{\partial x_{n,c_{in},h_{in},w_{in}}} = \sum_{c_{out}} \sum_{k_h=0}^{K_h-1} \sum_{k_w=0}^{K_w-1} \frac{\partial L}{\partial y_{n,c_{out},h_{out},w_{out}}} \cdot w_{c_{out},c_{in},k_h,k_w}$$</div>
    </div>
    
    <div class="formula">
        <h4>Weight Gradient Formula</h4>
        <div class="math-container">$$\frac{\partial L}{\partial w_{c_{out},c_{in},k_h,k_w}} = \sum_{n} \sum_{h_{out},w_{out}} \frac{\partial L}{\partial y_{n,c_{out},h_{out},w_{out}}} \cdot x_{n,c_{in},h_{in}+k_h,w_{in}+k_w}$$</div>
    </div>
    
    <div class="formula">
        <h4>Bias Gradient Formula</h4>
        <div class="math-container">$$\frac{\partial L}{\partial b_{c_{out}}} = \sum_{n} \sum_{h_{out},w_{out}} \frac{\partial L}{\partial y_{n,c_{out},h_{out},w_{out}}}$$</div>
    </div>
    </div>
    </div>

<div class='layer-summary'><h2>Layer: pool1 (MaxPool2d)</h2>

<p><strong>Input Shape</strong>: torch.Size([1, 2, 8, 8])</p>
<p><strong>Output Shape</strong>: torch.Size([1, 2, 4, 4])</p>

<h3>Forward Pass</h3>

<p><strong>General Formula</strong>: \(y_{n,c,h_{out},w_{out}} = \max_{k_h,k_w} x_{n,c,h_{in}+k_h,w_{in}+k_w}\)</p>


    <div class="computation-details">
    
    <h3>Computing output at position: batch = 0, channel = 0, y = 1, x = 1</h3>
    
    <h4>Input Receptive Field (Shape: (2, 2))</h4>
    <table class='tensor-table'>
<tr><th></th><th>[0]</th><th>[1]</th></tr>
<tr><th>[0]</th><td>1.2314</td><td>1.4557</td></tr>
<tr><th>[1]</th><td>1.7514</td><td>1.8643</td></tr>
</table>
    <p>Maximum value position in receptive field: (1, 1) with value 1.8643</p>
    
    <div class="formula">
        <h4>General Formula</h4>
        <div class="math-container">$$y_{0,0,1,1} = \max_{i,j \in \text{receptive field}} x_{0,0,i,j}$$</div>
    </div>
    
    <div class="formula">
        <h4>Value Substitution</h4>
        <div class="math-container">$$\max(1.2314, 1.4557, 1.7514, 1.8643)$$</div>
    </div>
    
    <h3>Computation Result</h3>
    <p>Calculated max value: 1.864286</p>
    <p>Actual output value: 1.864286</p>
    
</div><h3>Backward Pass</h3>

<p><strong>General Formula</strong>: \(\frac{\partial L}{\partial x_{n,c,h_{in},w_{in}}} = \begin{cases} \frac{\partial L}{\partial y_{n,c,h_{out},w_{out}}} & \text{if } x_{n,c,h_{in},w_{in}} \text{ is max in pool} \\ 0 & \text{otherwise} \end{cases}\)</p>


    <div class="backward-computation">
    <h3>Backward Computation for MaxPool2d at Position: batch = 0, channel = 0, y = 1, x = 1</h3>
    
    <h4>Gradient Output Value</h4>
    <p>-0.005817</p>
    
    <h4>Input Receptive Field</h4>
    <table class='tensor-table'>
<tr><th></th><th>[0]</th><th>[1]</th></tr>
<tr><th>[0]</th><td>1.2314</td><td>1.4557</td></tr>
<tr><th>[1]</th><td>1.7514</td><td>1.8643</td></tr>
</table>
    
    <p>Maximum value position in receptive field: (1, 1) with value 1.864286</p>
    
    <h4>Gradient Propagation to Input</h4>
    <table class="tensor-table">
    <tr><th>Position (y, x)</th><th>Formula</th><th>Gradient Value</th></tr>
    <tr><td>(3, 3)</td><td>$-0.0058$</td><td>-0.005817</td></tr>

    </table>
    
    <p>All other positions in the receptive field receive zero gradient.</p>
    
    <h4>Gradient Map (Receptive Field)</h4>
    <table class='tensor-table'>
<tr><th></th><th>[0]</th><th>[1]</th></tr>
<tr><th>[0]</th><td>0.0000</td><td>0.0000</td></tr>
<tr><th>[1]</th><td>0.0000</td><td>-0.0058</td></tr>
</table>
    <div class="formula">
        <h4>General Gradient Formula</h4>
        <div class="math-container">$$\frac{\partial L}{\partial x_{i,j}} = \begin{cases} \frac{\partial L}{\partial y} & \text{if } (i,j) \text{ is the location of max value} \\ 0 & \text{otherwise} \end{cases}$$</div>
    </div>
    </div>
    </div>

<div class='layer-summary'><h2>Layer: conv2 (Conv2d)</h2>

<p><strong>Input Shape</strong>: torch.Size([1, 2, 4, 4])</p>
<p><strong>Output Shape</strong>: torch.Size([1, 4, 4, 4])</p>

<h3>Forward Pass</h3>

<p><strong>General Formula</strong>: \(y_{n,c_{out},h_{out},w_{out}} = \sum_{c_{in},k_h,k_w} x_{n,c_{in},h_{in}+k_h,w_{in}+k_w} \cdot w_{c_{out},c_{in},k_h,k_w} + b_{c_{out}}\)</p>


    <div class="computation-details">
    
    <h3>Computing output at position: batch = 0, out_channel = 0, y = 1, x = 1</h3>
    
    <h4>Filter Weights (Shape: (2, 3, 3))</h4>
    <h5>Channel 0</h5>
<table class='tensor-table'>
<tr><th></th><th>[0]</th><th>[1]</th><th>[2]</th></tr>
<tr><th>[0]</th><td>-0.0121</td><td>-0.0530</td><td>0.1847</td></tr>
<tr><th>[1]</th><td>0.1785</td><td>-0.1867</td><td>-0.0138</td></tr>
<tr><th>[2]</th><td>-0.1293</td><td>0.2280</td><td>-0.0635</td></tr>
</table>
<h5>Channel 1</h5>
<table class='tensor-table'>
<tr><th></th><th>[0]</th><th>[1]</th><th>[2]</th></tr>
<tr><th>[0]</th><td>-0.1809</td><td>0.0908</td><td>0.0543</td></tr>
<tr><th>[1]</th><td>0.1646</td><td>-0.0593</td><td>-0.0835</td></tr>
<tr><th>[2]</th><td>0.2243</td><td>0.0444</td><td>0.0912</td></tr>
</table>

    
    <h4>Input Receptive Field (Shape: (3, 3, 2))</h4>
    <h5>Channel 0</h5>
<table class='tensor-table'>
<tr><th></th><th>[0]</th><th>[1]</th></tr>
<tr><th>[0]</th><td>0.5800</td><td>0.3629</td></tr>
<tr><th>[1]</th><td>1.3000</td><td>0.9214</td></tr>
<tr><th>[2]</th><td>1.3686</td><td>0.9700</td></tr>
</table>
<h5>Channel 1</h5>
<table class='tensor-table'>
<tr><th></th><th>[0]</th><th>[1]</th></tr>
<tr><th>[0]</th><td>1.4657</td><td>0.9514</td></tr>
<tr><th>[1]</th><td>1.8643</td><td>1.2243</td></tr>
<tr><th>[2]</th><td>2.2229</td><td>1.3929</td></tr>
</table>
<h5>Channel 2</h5>
<table class='tensor-table'>
<tr><th></th><th>[0]</th><th>[1]</th></tr>
<tr><th>[0]</th><td>1.2486</td><td>0.9400</td></tr>
<tr><th>[1]</th><td>2.0186</td><td>1.4814</td></tr>
<tr><th>[2]</th><td>1.9471</td><td>1.3814</td></tr>
</table>

    
    <div class="formula">
        <h4>General Formula</h4>
        <div class="math-container">$$y_{n,c_{out},h_{out},w_{out}} = \sum_{c_{in}} \sum_{k_h=0}^{K_h-1} \sum_{k_w=0}^{K_w-1} x_{n,c_{in},h_{in}+k_h,w_{in}+k_w} \cdot w_{c_{out},c_{in},k_h,k_w} + b_{c_{out}}$$</div>
    </div>
    
    <div class="formula">
        <h4>Value Substitution</h4>
        <div class="math-container">$$(0.5800 \times -0.0121) + (1.3000 \times -0.0530) + (1.3686 \times 0.1847) + (1.4657 \times 0.1785) + (1.8643 \times -0.1867) + (2.2229 \times -0.0138) + (1.2486 \times -0.1293) + (2.0186 \times 0.2280) + (1.9471 \times -0.0635) + (0.3629 \times -0.1809) + (0.9214 \times 0.0908) + (0.9700 \times 0.0543) + (0.9514 \times 0.1646) + (1.2243 \times -0.0593) + (1.3929 \times -0.0835) + (0.9400 \times 0.2243) + (1.4814 \times 0.0444) + (1.3814 \times 0.0912) + -0.1603$$</div>
    </div>
    
    <h3>Computation Result</h3>
    <p>Calculated value: 0.515430</p>
    <p>Actual output value: 0.515430</p>
    
</div><h3>Backward Pass</h3>

<p><strong>General Formula</strong>: \(\frac{\partial L}{\partial x_{n,c_{in},h_{in},w_{in}}} = \sum_{c_{out},k_h,k_w} \frac{\partial L}{\partial y_{n,c_{out},h_{out},w_{out}}} \cdot w_{c_{out},c_{in},k_h,k_w}\)</p>


    <div class="backward-computation">
    <h3>Backward Computation for Conv2d at Position: batch = 0, out_channel = 0, y = 1, x = 1</h3>
    
    <h4>Gradient Output Value</h4>
    <p>0.007880</p>
    
    <h4>Filter Weights</h4>
    <h5>Channel 0</h5>
<table class='tensor-table'>
<tr><th></th><th>[0]</th><th>[1]</th><th>[2]</th></tr>
<tr><th>[0]</th><td>-0.0121</td><td>-0.0530</td><td>0.1847</td></tr>
<tr><th>[1]</th><td>0.1785</td><td>-0.1867</td><td>-0.0138</td></tr>
<tr><th>[2]</th><td>-0.1293</td><td>0.2280</td><td>-0.0635</td></tr>
</table>
<h5>Channel 1</h5>
<table class='tensor-table'>
<tr><th></th><th>[0]</th><th>[1]</th><th>[2]</th></tr>
<tr><th>[0]</th><td>-0.1809</td><td>0.0908</td><td>0.0543</td></tr>
<tr><th>[1]</th><td>0.1646</td><td>-0.0593</td><td>-0.0835</td></tr>
<tr><th>[2]</th><td>0.2243</td><td>0.0444</td><td>0.0912</td></tr>
</table>

    
    <h4>Gradient Propagation to Input</h4>
    <table class="tensor-table">
    <tr><th>Position (y, x, c)</th><th>Formula</th><th>Gradient Value</th></tr>
    <tr><td>(0, 0, 0)</td><td>$(0.0079 \times -0.0121)$</td><td>-0.000095</td></tr>
<tr><td>(0, 1, 0)</td><td>$(0.0079 \times -0.0530)$</td><td>-0.000418</td></tr>
<tr><td>(0, 2, 0)</td><td>$(0.0079 \times 0.1847)$</td><td>0.001455</td></tr>
<tr><td>(1, 0, 0)</td><td>$(0.0079 \times 0.1785)$</td><td>0.001406</td></tr>
<tr><td>(1, 1, 0)</td><td>$(0.0079 \times -0.1867)$</td><td>-0.001471</td></tr>
<tr><td>(1, 2, 0)</td><td>$(0.0079 \times -0.0138)$</td><td>-0.000109</td></tr>
<tr><td>(2, 0, 0)</td><td>$(0.0079 \times -0.1293)$</td><td>-0.001019</td></tr>
<tr><td>(2, 1, 0)</td><td>$(0.0079 \times 0.2280)$</td><td>0.001797</td></tr>
<tr><td>(2, 2, 0)</td><td>$(0.0079 \times -0.0635)$</td><td>-0.000500</td></tr>
<tr><td>(0, 0, 1)</td><td>$(0.0079 \times -0.1809)$</td><td>-0.001425</td></tr>
<tr><td>(0, 1, 1)</td><td>$(0.0079 \times 0.0908)$</td><td>0.000715</td></tr>
<tr><td>(0, 2, 1)</td><td>$(0.0079 \times 0.0543)$</td><td>0.000428</td></tr>
<tr><td>(1, 0, 1)</td><td>$(0.0079 \times 0.1646)$</td><td>0.001297</td></tr>
<tr><td>(1, 1, 1)</td><td>$(0.0079 \times -0.0593)$</td><td>-0.000468</td></tr>
<tr><td>(1, 2, 1)</td><td>$(0.0079 \times -0.0835)$</td><td>-0.000658</td></tr>
<tr><td>(2, 0, 1)</td><td>$(0.0079 \times 0.2243)$</td><td>0.001768</td></tr>
<tr><td>(2, 1, 1)</td><td>$(0.0079 \times 0.0444)$</td><td>0.000349</td></tr>
<tr><td>(2, 2, 1)</td><td>$(0.0079 \times 0.0912)$</td><td>0.000719</td></tr>

    </table>
    
    <h4>Weight Gradient Computation</h4>
    <table class="tensor-table">
    <tr><th>Weight Position (out_c, in_c, ky, kx)</th><th>Formula</th><th>Gradient Value</th></tr>
    <tr><td>(0, 0, 0, 0)</td><td>$(0.0079 \times 0.5800)$</td><td>0.004570</td></tr>
<tr><td>(0, 0, 0, 1)</td><td>$(0.0079 \times 1.3000)$</td><td>0.010243</td></tr>
<tr><td>(0, 0, 0, 2)</td><td>$(0.0079 \times 1.3686)$</td><td>0.010784</td></tr>
<tr><td>(0, 0, 1, 0)</td><td>$(0.0079 \times 1.4657)$</td><td>0.011549</td></tr>
<tr><td>(0, 0, 1, 1)</td><td>$(0.0079 \times 1.8643)$</td><td>0.014690</td></tr>
<tr><td>(0, 0, 1, 2)</td><td>$(0.0079 \times 2.2229)$</td><td>0.017515</td></tr>
<tr><td>(0, 0, 2, 0)</td><td>$(0.0079 \times 1.2486)$</td><td>0.009838</td></tr>
<tr><td>(0, 0, 2, 1)</td><td>$(0.0079 \times 2.0186)$</td><td>0.015905</td></tr>
<tr><td>(0, 0, 2, 2)</td><td>$(0.0079 \times 1.9471)$</td><td>0.015343</td></tr>
<tr><td>(0, 1, 0, 0)</td><td>$(0.0079 \times 0.3629)$</td><td>0.002859</td></tr>
<tr><td>(0, 1, 0, 1)</td><td>$(0.0079 \times 0.9214)$</td><td>0.007260</td></tr>
<tr><td>(0, 1, 0, 2)</td><td>$(0.0079 \times 0.9700)$</td><td>0.007643</td></tr>
<tr><td>(0, 1, 1, 0)</td><td>$(0.0079 \times 0.9514)$</td><td>0.007497</td></tr>
<tr><td>(0, 1, 1, 1)</td><td>$(0.0079 \times 1.2243)$</td><td>0.009647</td></tr>
<tr><td>(0, 1, 1, 2)</td><td>$(0.0079 \times 1.3929)$</td><td>0.010975</td></tr>
<tr><td>(0, 1, 2, 0)</td><td>$(0.0079 \times 0.9400)$</td><td>0.007407</td></tr>
<tr><td>(0, 1, 2, 1)</td><td>$(0.0079 \times 1.4814)$</td><td>0.011673</td></tr>
<tr><td>(0, 1, 2, 2)</td><td>$(0.0079 \times 1.3814)$</td><td>0.010885</td></tr>

    </table>
    
    <h4>Bias Gradient Computation</h4>
    <p>Bias Gradient for output channel 0: 0.007880</p>
    
    <h3>General Gradient Formulas</h3>
    
    <div class="formula">
        <h4>Input Gradient Formula</h4>
        <div class="math-container">$$\frac{\partial L}{\partial x_{n,c_{in},h_{in},w_{in}}} = \sum_{c_{out}} \sum_{k_h=0}^{K_h-1} \sum_{k_w=0}^{K_w-1} \frac{\partial L}{\partial y_{n,c_{out},h_{out},w_{out}}} \cdot w_{c_{out},c_{in},k_h,k_w}$$</div>
    </div>
    
    <div class="formula">
        <h4>Weight Gradient Formula</h4>
        <div class="math-container">$$\frac{\partial L}{\partial w_{c_{out},c_{in},k_h,k_w}} = \sum_{n} \sum_{h_{out},w_{out}} \frac{\partial L}{\partial y_{n,c_{out},h_{out},w_{out}}} \cdot x_{n,c_{in},h_{in}+k_h,w_{in}+k_w}$$</div>
    </div>
    
    <div class="formula">
        <h4>Bias Gradient Formula</h4>
        <div class="math-container">$$\frac{\partial L}{\partial b_{c_{out}}} = \sum_{n} \sum_{h_{out},w_{out}} \frac{\partial L}{\partial y_{n,c_{out},h_{out},w_{out}}}$$</div>
    </div>
    </div>
    </div>

<div class='layer-summary'><h2>Layer: pool2 (AvgPool2d)</h2>

<p><strong>Input Shape</strong>: torch.Size([1, 4, 4, 4])</p>
<p><strong>Output Shape</strong>: torch.Size([1, 4, 2, 2])</p>

<h3>Forward Pass</h3>

<p><strong>General Formula</strong>: \(y_{n,c,h_{out},w_{out}} = \frac{1}{k_h \cdot k_w} \sum_{i=0}^{k_h-1} \sum_{j=0}^{k_w-1} x_{n,c,h_{in}+i,w_{in}+j}\)</p>


    <div class="computation-details">
    
    <h3>Computing output at position: batch = 0, channel = 0, y = 1, x = 1</h3>
    
    <h4>Input Receptive Field (Shape: (2, 2))</h4>
    <table class='tensor-table'>
<tr><th></th><th>[0]</th><th>[1]</th></tr>
<tr><th>[0]</th><td>0.5805</td><td>0.2434</td></tr>
<tr><th>[1]</th><td>0.0000</td><td>0.0000</td></tr>
</table>
    <p>Kernel size: (2, 2)</p>
    
    <div class="formula">
        <h4>General Formula</h4>
        <div class="math-container">$$y_{0,0,1,1} = \frac{1}{(2, 2) \times (2, 2)} \sum_{i=0}^{(2, 2)-1} \sum_{j=0}^{(2, 2)-1} x_{0,0,1+i,1+j}$$</div>
    </div>
    
    <div class="formula">
        <h4>Value Substitution</h4>
        <div class="math-container">$$\frac1{2 \times 2} \times (0.5805 + 0.2434 + 0.0000 + 0.0000)$$</div>
    </div>
    
    <h3>Computation Result</h3>
    <p>Calculated average value: 0.205988</p>
    <p>Actual output value: 0.205988</p>
    
</div><h3>Backward Pass</h3>

<p><strong>General Formula</strong>: \(\frac{\partial L}{\partial x_{n,c,h_{in},w_{in}}} = \frac{1}{k_h \cdot k_w} \sum_{h_{out},w_{out}} \frac{\partial L}{\partial y_{n,c,h_{out},w_{out}}}\)</p>


    <div class="backward-computation">
    <h3>Backward Computation for AvgPool2d at Position: batch = 0, channel = 0, y = 1, x = 1</h3>
    
    <h4>Gradient Output Value</h4>
    <p>-0.004265</p>
    
    <p>Pool Size: 4 (kernel: (2, 2))</p>
    <p>Distributed Gradient: -0.001066 (-0.004265 / 4)</p>
    
    <h4>Gradient Propagation to Input</h4>
    <table class="tensor-table">
    <tr><th>Position (y, x)</th><th>Formula</th><th>Gradient Value</th></tr>
    <tr><td>(2, 2)</td><td>$-0.0043 / 4$</td><td>-0.001066</td></tr>
<tr><td>(2, 3)</td><td>$-0.0043 / 4$</td><td>-0.001066</td></tr>
<tr><td>(3, 2)</td><td>$-0.0043 / 4$</td><td>-0.001066</td></tr>
<tr><td>(3, 3)</td><td>$-0.0043 / 4$</td><td>-0.001066</td></tr>

    </table>
    
    <h4>Gradient Map (Receptive Field)</h4>
    <table class='tensor-table'>
<tr><th></th><th>[0]</th><th>[1]</th></tr>
<tr><th>[0]</th><td>-0.0011</td><td>-0.0011</td></tr>
<tr><th>[1]</th><td>-0.0011</td><td>-0.0011</td></tr>
</table>
    <div class="formula">
        <h4>General Gradient Formula</h4>
        <div class="math-container">$$\frac{\partial L}{\partial x_{i,j}} = \frac{1}{(2, 2)^2} \cdot \frac{\partial L}{\partial y} \quad \text{for all } (i,j) \text{ in the receptive field}$$</div>
    </div>
    </div>
    </div>

<div class='layer-summary'><h2>Layer: conv3 (Conv2d)</h2>

<p><strong>Input Shape</strong>: torch.Size([1, 4, 2, 2])</p>
<p><strong>Output Shape</strong>: torch.Size([1, 2, 2, 2])</p>

<h3>Forward Pass</h3>

<p><strong>General Formula</strong>: \(y_{n,c_{out},h_{out},w_{out}} = \sum_{c_{in},k_h,k_w} x_{n,c_{in},h_{in}+k_h,w_{in}+k_w} \cdot w_{c_{out},c_{in},k_h,k_w} + b_{c_{out}}\)</p>


    <div class="computation-details">
    
    <h3>Computing output at position: batch = 0, out_channel = 0, y = 1, x = 1</h3>
    
    <h4>Filter Weights (Shape: (4, 1, 1))</h4>
    <h5>Channel 0</h5>
<table class='tensor-table'>
<tr><th></th><th>[0]</th></tr>
<tr><th>[0]</th><td>-0.3695</td></tr>
</table>
<h5>Channel 1</h5>
<table class='tensor-table'>
<tr><th></th><th>[0]</th></tr>
<tr><th>[0]</th><td>0.4977</td></tr>
</table>
<h5>Channel 2</h5>
<table class='tensor-table'>
<tr><th></th><th>[0]</th></tr>
<tr><th>[0]</th><td>-0.4407</td></tr>
</table>
<h5>Channel 3</h5>
<table class='tensor-table'>
<tr><th></th><th>[0]</th></tr>
<tr><th>[0]</th><td>0.0024</td></tr>
</table>

    
    <h4>Input Receptive Field (Shape: (1, 1, 4))</h4>
    <h5>Channel 0</h5>
<table class='tensor-table'>
<tr><th></th><th>[0]</th><th>[1]</th><th>[2]</th><th>[3]</th></tr>
<tr><th>[0]</th><td>0.2060</td><td>0.9744</td><td>0.0562</td><td>1.2412</td></tr>
</table>

    
    <div class="formula">
        <h4>General Formula</h4>
        <div class="math-container">$$y_{n,c_{out},h_{out},w_{out}} = \sum_{c_{in}} \sum_{k_h=0}^{K_h-1} \sum_{k_w=0}^{K_w-1} x_{n,c_{in},h_{in}+k_h,w_{in}+k_w} \cdot w_{c_{out},c_{in},k_h,k_w} + b_{c_{out}}$$</div>
    </div>
    
    <div class="formula">
        <h4>Value Substitution</h4>
        <div class="math-container">$$(0.2060 \times -0.3695) + (0.9744 \times 0.4977) + (0.0562 \times -0.4407) + (1.2412 \times 0.0024) + -0.3224$$</div>
    </div>
    
    <h3>Computation Result</h3>
    <p>Calculated value: 0.064689</p>
    <p>Actual output value: 0.064689</p>
    
</div><h3>Backward Pass</h3>

<p><strong>General Formula</strong>: \(\frac{\partial L}{\partial x_{n,c_{in},h_{in},w_{in}}} = \sum_{c_{out},k_h,k_w} \frac{\partial L}{\partial y_{n,c_{out},h_{out},w_{out}}} \cdot w_{c_{out},c_{in},k_h,k_w}\)</p>


    <div class="backward-computation">
    <h3>Backward Computation for Conv2d at Position: batch = 0, out_channel = 0, y = 1, x = 1</h3>
    
    <h4>Gradient Output Value</h4>
    <p>-0.066347</p>
    
    <h4>Filter Weights</h4>
    <h5>Channel 0</h5>
<table class='tensor-table'>
<tr><th></th><th>[0]</th></tr>
<tr><th>[0]</th><td>-0.3695</td></tr>
</table>
<h5>Channel 1</h5>
<table class='tensor-table'>
<tr><th></th><th>[0]</th></tr>
<tr><th>[0]</th><td>0.4977</td></tr>
</table>
<h5>Channel 2</h5>
<table class='tensor-table'>
<tr><th></th><th>[0]</th></tr>
<tr><th>[0]</th><td>-0.4407</td></tr>
</table>
<h5>Channel 3</h5>
<table class='tensor-table'>
<tr><th></th><th>[0]</th></tr>
<tr><th>[0]</th><td>0.0024</td></tr>
</table>

    
    <h4>Gradient Propagation to Input</h4>
    <table class="tensor-table">
    <tr><th>Position (y, x, c)</th><th>Formula</th><th>Gradient Value</th></tr>
    <tr><td>(1, 1, 0)</td><td>$(-0.0663 \times -0.3695)$</td><td>0.024514</td></tr>
<tr><td>(1, 1, 1)</td><td>$(-0.0663 \times 0.4977)$</td><td>-0.033019</td></tr>
<tr><td>(1, 1, 2)</td><td>$(-0.0663 \times -0.4407)$</td><td>0.029240</td></tr>
<tr><td>(1, 1, 3)</td><td>$(-0.0663 \times 0.0024)$</td><td>-0.000159</td></tr>

    </table>
    
    <h4>Weight Gradient Computation</h4>
    <table class="tensor-table">
    <tr><th>Weight Position (out_c, in_c, ky, kx)</th><th>Formula</th><th>Gradient Value</th></tr>
    <tr><td>(0, 0, 0, 0)</td><td>$(-0.0663 \times 0.2060)$</td><td>-0.013667</td></tr>
<tr><td>(0, 1, 0, 0)</td><td>$(-0.0663 \times 0.9744)$</td><td>-0.064652</td></tr>
<tr><td>(0, 2, 0, 0)</td><td>$(-0.0663 \times 0.0562)$</td><td>-0.003726</td></tr>
<tr><td>(0, 3, 0, 0)</td><td>$(-0.0663 \times 1.2412)$</td><td>-0.082351</td></tr>

    </table>
    
    <h4>Bias Gradient Computation</h4>
    <p>Bias Gradient for output channel 0: -0.066347</p>
    
    <h3>General Gradient Formulas</h3>
    
    <div class="formula">
        <h4>Input Gradient Formula</h4>
        <div class="math-container">$$\frac{\partial L}{\partial x_{n,c_{in},h_{in},w_{in}}} = \sum_{c_{out}} \sum_{k_h=0}^{K_h-1} \sum_{k_w=0}^{K_w-1} \frac{\partial L}{\partial y_{n,c_{out},h_{out},w_{out}}} \cdot w_{c_{out},c_{in},k_h,k_w}$$</div>
    </div>
    
    <div class="formula">
        <h4>Weight Gradient Formula</h4>
        <div class="math-container">$$\frac{\partial L}{\partial w_{c_{out},c_{in},k_h,k_w}} = \sum_{n} \sum_{h_{out},w_{out}} \frac{\partial L}{\partial y_{n,c_{out},h_{out},w_{out}}} \cdot x_{n,c_{in},h_{in}+k_h,w_{in}+k_w}$$</div>
    </div>
    
    <div class="formula">
        <h4>Bias Gradient Formula</h4>
        <div class="math-container">$$\frac{\partial L}{\partial b_{c_{out}}} = \sum_{n} \sum_{h_{out},w_{out}} \frac{\partial L}{\partial y_{n,c_{out},h_{out},w_{out}}}$$</div>
    </div>
    </div>
    </div>

<div class='layer-summary'><h2>Layer: conv_out (Conv2d)</h2>

<p><strong>Input Shape</strong>: torch.Size([1, 2, 2, 2])</p>
<p><strong>Output Shape</strong>: torch.Size([1, 2, 1, 1])</p>

<h3>Forward Pass</h3>

<p><strong>General Formula</strong>: \(y_{n,c_{out},h_{out},w_{out}} = \sum_{c_{in},k_h,k_w} x_{n,c_{in},h_{in}+k_h,w_{in}+k_w} \cdot w_{c_{out},c_{in},k_h,k_w} + b_{c_{out}}\)</p>


    <div class="computation-details">
    
    <h3>Computing output at position: batch = 0, out_channel = 0, y = 0, x = 0</h3>
    
    <h4>Filter Weights (Shape: (2, 2, 2))</h4>
    <h5>Channel 0</h5>
<table class='tensor-table'>
<tr><th></th><th>[0]</th><th>[1]</th></tr>
<tr><th>[0]</th><td>0.2991</td><td>0.2996</td></tr>
<tr><th>[1]</th><td>0.1567</td><td>0.2650</td></tr>
</table>
<h5>Channel 1</h5>
<table class='tensor-table'>
<tr><th></th><th>[0]</th><th>[1]</th></tr>
<tr><th>[0]</th><td>-0.3454</td><td>-0.2573</td></tr>
<tr><th>[1]</th><td>0.1141</td><td>0.2871</td></tr>
</table>

    
    <h4>Input Receptive Field (Shape: (2, 2, 2))</h4>
    <h5>Channel 0</h5>
<table class='tensor-table'>
<tr><th></th><th>[0]</th><th>[1]</th></tr>
<tr><th>[0]</th><td>0.0000</td><td>0.3112</td></tr>
<tr><th>[1]</th><td>0.0166</td><td>0.6324</td></tr>
</table>
<h5>Channel 1</h5>
<table class='tensor-table'>
<tr><th></th><th>[0]</th><th>[1]</th></tr>
<tr><th>[0]</th><td>0.0000</td><td>0.3118</td></tr>
<tr><th>[1]</th><td>0.0647</td><td>0.5764</td></tr>
</table>

    
    <div class="formula">
        <h4>General Formula</h4>
        <div class="math-container">$$y_{n,c_{out},h_{out},w_{out}} = \sum_{c_{in}} \sum_{k_h=0}^{K_h-1} \sum_{k_w=0}^{K_w-1} x_{n,c_{in},h_{in}+k_h,w_{in}+k_w} \cdot w_{c_{out},c_{in},k_h,k_w} + b_{c_{out}}$$</div>
    </div>
    
    <div class="formula">
        <h4>Value Substitution</h4>
        <div class="math-container">$$(0.0000 \times 0.2991) + (0.0166 \times 0.2996) + (0.0000 \times 0.1567) + (0.0647 \times 0.2650) + (0.3112 \times -0.3454) + (0.6324 \times -0.2573) + (0.3118 \times 0.1141) + (0.5764 \times 0.2871) + 0.1359$$</div>
    </div>
    
    <h3>Computation Result</h3>
    <p>Calculated value: 0.088895</p>
    <p>Actual output value: 0.088895</p>
    
</div><h3>Backward Pass</h3>

<p><strong>General Formula</strong>: \(\frac{\partial L}{\partial x_{n,c_{in},h_{in},w_{in}}} = \sum_{c_{out},k_h,k_w} \frac{\partial L}{\partial y_{n,c_{out},h_{out},w_{out}}} \cdot w_{c_{out},c_{in},k_h,k_w}\)</p>


    <div class="backward-computation">
    <h3>Backward Computation for Conv2d at Position: batch = 0, out_channel = 0, y = 0, x = 0</h3>
    
    <h4>Gradient Output Value</h4>
    <p>-0.247796</p>
    
    <h4>Filter Weights</h4>
    <h5>Channel 0</h5>
<table class='tensor-table'>
<tr><th></th><th>[0]</th><th>[1]</th></tr>
<tr><th>[0]</th><td>0.2991</td><td>0.2996</td></tr>
<tr><th>[1]</th><td>0.1567</td><td>0.2650</td></tr>
</table>
<h5>Channel 1</h5>
<table class='tensor-table'>
<tr><th></th><th>[0]</th><th>[1]</th></tr>
<tr><th>[0]</th><td>-0.3454</td><td>-0.2573</td></tr>
<tr><th>[1]</th><td>0.1141</td><td>0.2871</td></tr>
</table>

    
    <h4>Gradient Propagation to Input</h4>
    <table class="tensor-table">
    <tr><th>Position (y, x, c)</th><th>Formula</th><th>Gradient Value</th></tr>
    <tr><td>(0, 0, 0)</td><td>$(-0.2478 \times 0.2991)$</td><td>-0.074117</td></tr>
<tr><td>(0, 1, 0)</td><td>$(-0.2478 \times 0.2996)$</td><td>-0.074241</td></tr>
<tr><td>(1, 0, 0)</td><td>$(-0.2478 \times 0.1567)$</td><td>-0.038830</td></tr>
<tr><td>(1, 1, 0)</td><td>$(-0.2478 \times 0.2650)$</td><td>-0.065670</td></tr>
<tr><td>(0, 0, 1)</td><td>$(-0.2478 \times -0.3454)$</td><td>0.085585</td></tr>
<tr><td>(0, 1, 1)</td><td>$(-0.2478 \times -0.2573)$</td><td>0.063764</td></tr>
<tr><td>(1, 0, 1)</td><td>$(-0.2478 \times 0.1141)$</td><td>-0.028270</td></tr>
<tr><td>(1, 1, 1)</td><td>$(-0.2478 \times 0.2871)$</td><td>-0.071139</td></tr>

    </table>
    
    <h4>Weight Gradient Computation</h4>
    <table class="tensor-table">
    <tr><th>Weight Position (out_c, in_c, ky, kx)</th><th>Formula</th><th>Gradient Value</th></tr>
    <tr><td>(0, 0, 0, 0)</td><td>$(-0.2478 \times 0.0000)$</td><td>-0.000000</td></tr>
<tr><td>(0, 0, 0, 1)</td><td>$(-0.2478 \times 0.0166)$</td><td>-0.004123</td></tr>
<tr><td>(0, 0, 1, 0)</td><td>$(-0.2478 \times 0.0000)$</td><td>-0.000000</td></tr>
<tr><td>(0, 0, 1, 1)</td><td>$(-0.2478 \times 0.0647)$</td><td>-0.016030</td></tr>
<tr><td>(0, 1, 0, 0)</td><td>$(-0.2478 \times 0.3112)$</td><td>-0.077112</td></tr>
<tr><td>(0, 1, 0, 1)</td><td>$(-0.2478 \times 0.6324)$</td><td>-0.156710</td></tr>
<tr><td>(0, 1, 1, 0)</td><td>$(-0.2478 \times 0.3118)$</td><td>-0.077269</td></tr>
<tr><td>(0, 1, 1, 1)</td><td>$(-0.2478 \times 0.5764)$</td><td>-0.142822</td></tr>

    </table>
    
    <h4>Bias Gradient Computation</h4>
    <p>Bias Gradient for output channel 0: -0.247796</p>
    
    <h3>General Gradient Formulas</h3>
    
    <div class="formula">
        <h4>Input Gradient Formula</h4>
        <div class="math-container">$$\frac{\partial L}{\partial x_{n,c_{in},h_{in},w_{in}}} = \sum_{c_{out}} \sum_{k_h=0}^{K_h-1} \sum_{k_w=0}^{K_w-1} \frac{\partial L}{\partial y_{n,c_{out},h_{out},w_{out}}} \cdot w_{c_{out},c_{in},k_h,k_w}$$</div>
    </div>
    
    <div class="formula">
        <h4>Weight Gradient Formula</h4>
        <div class="math-container">$$\frac{\partial L}{\partial w_{c_{out},c_{in},k_h,k_w}} = \sum_{n} \sum_{h_{out},w_{out}} \frac{\partial L}{\partial y_{n,c_{out},h_{out},w_{out}}} \cdot x_{n,c_{in},h_{in}+k_h,w_{in}+k_w}$$</div>
    </div>
    
    <div class="formula">
        <h4>Bias Gradient Formula</h4>
        <div class="math-container">$$\frac{\partial L}{\partial b_{c_{out}}} = \sum_{n} \sum_{h_{out},w_{out}} \frac{\partial L}{\partial y_{n,c_{out},h_{out},w_{out}}}$$</div>
    </div>
    </div>
    </div>
</body>
</html>